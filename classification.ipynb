{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-3.06751441e+00 -3.85245611e+00 -3.99767733e+00 ...  3.64459580e+00\n",
      "    5.85016810e+00  8.58712240e-01]\n",
      "  [-2.12816551e+00 -7.33478682e+00 -5.66763569e+00 ...  6.44525500e+00\n",
      "    7.82252989e+00  1.54853237e+00]\n",
      "  [-9.40460450e-01 -5.16861466e+00 -4.90839086e+00 ...  1.84716665e+00\n",
      "    3.09268271e+00  7.49362983e-01]\n",
      "  ...\n",
      "  [-1.88062175e+00 -6.48940075e+00 -5.28206937e+00 ...  2.72900614e+00\n",
      "    3.50771380e+00  1.97587707e-01]\n",
      "  [-1.66484834e+00 -8.47699973e+00 -7.08824371e+00 ...  4.40881470e+00\n",
      "    4.17768486e+00 -1.18187710e+00]\n",
      "  [-5.20890373e-01 -9.06242764e+00 -8.51753741e+00 ...  1.86744200e-01\n",
      "    1.65941363e+00  8.80486005e-01]]\n",
      "\n",
      " [[ 4.30215614e-01  2.55025648e+00  1.47753074e+00 ... -2.94856250e+00\n",
      "   -1.77925923e+00  4.42903685e-01]\n",
      "  [ 1.77790740e+00  4.84882221e+00  3.19347718e+00 ... -4.35634631e+00\n",
      "   -3.82876159e+00  4.32402370e-01]\n",
      "  [ 1.00342290e+00 -2.85375914e+00 -3.03067386e+00 ... -3.50510970e-02\n",
      "   -7.37486891e-01  2.56225568e-01]\n",
      "  ...\n",
      "  [ 1.78318248e+00  1.29515346e+01  1.08102416e+01 ... -3.32001620e+00\n",
      "   -3.00679865e+00  8.26957684e-02]\n",
      "  [ 1.12837904e+00  1.55383878e+01  1.37756129e+01 ...  3.14196355e+00\n",
      "    1.41016195e+00 -4.23428705e-01]\n",
      "  [-8.45653541e-01 -1.25052491e+01 -1.05808158e+01 ...  1.63168596e+01\n",
      "    1.00298439e+01  2.57201529e-01]]\n",
      "\n",
      " [[-1.48529377e-01 -8.34926686e+00 -9.98498985e+00 ... -2.55639142e+00\n",
      "   -2.46196663e+00 -7.24073463e-02]\n",
      "  [-5.83111976e-02 -1.34280181e+01 -1.65851802e+01 ... -1.85567967e+00\n",
      "   -1.78469562e+00 -3.97908826e-01]\n",
      "  [-1.48696607e+00 -6.27659576e+00 -1.08387995e+01 ... -4.61268469e+00\n",
      "   -4.17778842e+00 -4.83099637e-01]\n",
      "  ...\n",
      "  [ 1.89501690e-01  3.54032592e-01  1.07869532e+00 ... -1.20797684e+00\n",
      "   -1.32081372e+00 -3.49348436e-01]\n",
      "  [-4.66835278e-01 -3.99802176e-01  7.68111300e-03 ... -1.50795634e+00\n",
      "   -1.67826309e+00 -4.76967428e-01]\n",
      "  [-1.81419993e+00 -1.15153810e+01 -1.14887938e+01 ...  6.17910550e+00\n",
      "    4.53945273e+00 -6.38419206e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-4.88717952e-01  4.27558340e+00  5.04332333e+00 ...  3.10467405e-01\n",
      "    2.82542511e+00 -2.73326081e-01]\n",
      "  [-5.00508152e-01 -4.53835307e-01 -1.52664880e+00 ... -6.39565955e+00\n",
      "   -9.69121726e-01 -7.90715695e-01]\n",
      "  [ 1.24023446e-01 -3.95808847e+00 -7.57823214e+00 ... -5.98978693e+00\n",
      "   -6.98437310e-01  1.30107711e-01]\n",
      "  ...\n",
      "  [-8.03122788e-01 -3.30233676e+00 -2.40413870e+00 ... -6.02445516e+00\n",
      "   -8.00724151e-01 -6.30697541e-01]\n",
      "  [-6.15674040e-01 -3.21934106e+00 -2.31653737e+00 ... -9.64689734e+00\n",
      "   -4.94441853e+00 -9.37129638e-01]\n",
      "  [ 2.90843433e-01 -5.15250916e+00 -4.89922126e+00 ... -9.82754827e+00\n",
      "   -4.21943378e+00 -1.00887879e-01]]\n",
      "\n",
      " [[ 1.02650036e+00 -4.38384646e+00 -7.01177479e+00 ...  9.84810353e+00\n",
      "    6.26843957e+00  2.65306717e-01]\n",
      "  [ 5.89389254e-01 -5.68954876e+00 -1.03820707e+01 ...  1.47443449e+01\n",
      "    9.03925113e+00  5.21634743e-01]\n",
      "  [-2.20323255e-01 -1.97000685e+00 -7.02271816e+00 ...  1.17949537e+01\n",
      "    4.69934314e+00  4.04476063e-01]\n",
      "  ...\n",
      "  [ 2.03611543e+00 -1.17801839e+01 -1.38190916e+01 ...  1.24989801e+01\n",
      "    1.11605001e+01 -7.10330088e-02]\n",
      "  [ 2.84306044e+00 -1.33076167e+01 -1.45456674e+01 ...  8.78352925e+00\n",
      "    9.22758831e+00  5.57756344e-02]\n",
      "  [ 3.67048573e-01 -2.86482337e+00 -9.06923268e+00 ...  1.57128176e+01\n",
      "    9.26551766e+00  5.95740958e-01]]\n",
      "\n",
      " [[-2.58245691e+00  7.23751799e+00  3.53744501e+00 ... -1.68376437e+01\n",
      "   -1.11883377e+01  2.15097314e+00]\n",
      "  [-2.79445134e+00  9.36514264e+00  4.71387188e+00 ... -9.33355131e+00\n",
      "   -3.11702977e+00  1.55446983e+00]\n",
      "  [ 3.34944287e-01  2.04651615e+00 -4.25598601e-01 ... -7.01335478e+00\n",
      "   -3.08808900e+00 -3.18924614e-01]\n",
      "  ...\n",
      "  [-3.29080042e+00  1.15704429e+01  4.79395500e+00 ... -8.60195493e+00\n",
      "   -3.31572944e+00  1.28470727e+00]\n",
      "  [-3.75858218e+00  1.21865062e+01  4.94177059e+00 ... -7.93361325e+00\n",
      "   -3.05126056e+00  1.57662285e+00]\n",
      "  [-1.88248820e+00 -1.44331857e-01 -4.81567156e+00 ... -9.30765004e+00\n",
      "   -5.32657897e+00  5.26789744e-02]]]\n",
      "    subject_no  t1_math  t1_mirror  t1_stroop  t2_math  t2_mirror  t2_stroop  \\\n",
      "1        False     True      False      False     True      False      False   \n",
      "2        False    False      False      False    False      False      False   \n",
      "3        False    False      False      False    False      False      False   \n",
      "4        False    False      False      False    False      False      False   \n",
      "5        False     True       True       True    False      False      False   \n",
      "6         True    False      False      False    False      False      False   \n",
      "7         True    False      False      False    False      False      False   \n",
      "8         True    False      False      False     True       True      False   \n",
      "9         True    False       True      False    False      False      False   \n",
      "10        True    False      False      False    False      False      False   \n",
      "11        True     True      False      False     True      False      False   \n",
      "12        True    False       True      False    False      False      False   \n",
      "13        True    False      False      False    False      False      False   \n",
      "14        True    False      False      False     True      False      False   \n",
      "15        True     True       True      False     True      False      False   \n",
      "16        True    False      False      False    False       True      False   \n",
      "17        True     True       True       True     True       True      False   \n",
      "18        True     True      False       True    False       True      False   \n",
      "19        True    False       True       True    False      False      False   \n",
      "20        True     True       True       True     True       True      False   \n",
      "21        True     True       True       True     True       True       True   \n",
      "22        True     True       True      False     True       True       True   \n",
      "23        True    False      False      False    False      False      False   \n",
      "24        True    False      False      False    False      False      False   \n",
      "25        True    False      False      False    False      False      False   \n",
      "26        True     True       True       True     True       True       True   \n",
      "27        True    False      False      False    False      False      False   \n",
      "28        True     True       True       True     True       True      False   \n",
      "29        True     True       True       True     True       True       True   \n",
      "30        True     True       True      False     True       True      False   \n",
      "31        True     True       True      False     True       True      False   \n",
      "32        True    False      False      False    False      False      False   \n",
      "33        True    False      False      False    False      False      False   \n",
      "34        True    False      False      False    False      False      False   \n",
      "35        True     True      False      False    False      False      False   \n",
      "36        True    False      False       True    False      False      False   \n",
      "37        True     True      False      False    False      False      False   \n",
      "38        True     True      False      False    False       True      False   \n",
      "39        True     True      False      False    False      False      False   \n",
      "40        True    False      False      False    False       True      False   \n",
      "\n",
      "    t3_math  t3_mirror  t3_stroop  \n",
      "1     False       True      False  \n",
      "2      True      False      False  \n",
      "3      True       True      False  \n",
      "4      True      False      False  \n",
      "5     False       True      False  \n",
      "6      True       True      False  \n",
      "7      True      False       True  \n",
      "8     False       True      False  \n",
      "9      True      False       True  \n",
      "10     True      False      False  \n",
      "11    False       True      False  \n",
      "12     True       True      False  \n",
      "13    False      False      False  \n",
      "14     True      False      False  \n",
      "15     True      False      False  \n",
      "16     True      False       True  \n",
      "17     True       True       True  \n",
      "18     True       True       True  \n",
      "19    False      False      False  \n",
      "20     True       True       True  \n",
      "21     True       True       True  \n",
      "22     True       True      False  \n",
      "23     True      False      False  \n",
      "24    False       True      False  \n",
      "25    False       True      False  \n",
      "26    False      False       True  \n",
      "27    False      False      False  \n",
      "28     True      False      False  \n",
      "29     True       True      False  \n",
      "30     True      False      False  \n",
      "31     True      False       True  \n",
      "32     True       True      False  \n",
      "33     True      False       True  \n",
      "34    False      False      False  \n",
      "35    False       True      False  \n",
      "36    False      False      False  \n",
      "37     True      False       True  \n",
      "38    False      False      False  \n",
      "39     True      False      False  \n",
      "40    False      False      False  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import variables as v\n",
    "\n",
    "def load_dataset(data_type=\"ica_filtered\", test_type=\"Arithmetic\"):\n",
    "    \n",
    "    assert (test_type in v.TEST_TYPES)\n",
    "\n",
    "    assert (data_type in v.DATA_TYPES)\n",
    "\n",
    "    if data_type == \"ica_filtered\" and test_type != \"Arithmetic\":\n",
    "        print(\"Data of type\", data_type, \"does not have test type\", test_type)\n",
    "        return 0\n",
    "\n",
    "    if data_type == \"raw\":\n",
    "        dir = v.DIR_RAW\n",
    "        data_key = 'Data'\n",
    "    elif data_type == \"wt_filtered\":\n",
    "        dir = v.DIR_FILTERED\n",
    "        data_key = 'Clean_data'\n",
    "    else:\n",
    "        dir = v.DIR_ICA_FILTERED\n",
    "        data_key = 'Clean_data'\n",
    "        \n",
    "    dataset = np.empty((120, 32, 3200))\n",
    "\n",
    "    counter = 0\n",
    "    for filename in os.listdir(dir):\n",
    "        if test_type not in filename:\n",
    "            continue\n",
    "\n",
    "        f = os.path.join(dir, filename)\n",
    "        data = scipy.io.loadmat(f)[data_key]\n",
    "        dataset[counter] = data\n",
    "        counter += 1\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_labels():\n",
    "    \n",
    "    labels = pd.read_excel(v.LABELS_PATH)\n",
    "    labels = labels.rename(columns=v.COLUMNS_TO_RENAME)\n",
    "    labels = labels[1:]\n",
    "    labels = labels.astype(\"int\")\n",
    "    labels = labels > 5\n",
    "    return labels\n",
    "\n",
    "def format_labels(labels, test_type=\"Arithmetic\", epochs=1):\n",
    "    \n",
    "    assert (test_type in v.TEST_TYPES)\n",
    "\n",
    "    formatted_labels = []\n",
    "    for trial in v.TEST_TYPE_COLUMNS[test_type]:\n",
    "        formatted_labels.append(labels[trial])\n",
    "\n",
    "    formatted_labels = pd.concat(formatted_labels).to_numpy()\n",
    "\n",
    "    formatted_labels = formatted_labels.repeat(epochs)\n",
    "\n",
    "    return formatted_labels\n",
    "\n",
    "def split_data(data, sfreq):\n",
    "\n",
    "    n_trials, n_channels, n_samples = data.shape\n",
    "\n",
    "    epoched_data = np.empty((n_trials, n_samples//sfreq, n_channels, sfreq))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[2]//sfreq):\n",
    "            epoched_data[i, j] = data[i, :, j*sfreq:(j+1)*sfreq]\n",
    "    return epoched_data\n",
    "\n",
    "\n",
    "print(load_dataset())\n",
    "\n",
    "print(load_labels())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mne-features\n",
      "  Downloading mne_features-0.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from mne-features) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from mne-features) (1.11.4)\n",
      "Requirement already satisfied: numba in /opt/anaconda3/lib/python3.11/site-packages (from mne-features) (0.59.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (from mne-features) (1.2.2)\n",
      "Collecting mne (from mne-features)\n",
      "  Downloading mne-1.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyWavelets in /opt/anaconda3/lib/python3.11/site-packages (from mne-features) (1.5.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from mne-features) (2.1.4)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from mne->mne-features) (3.8.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from mne->mne-features) (4.65.0)\n",
      "Collecting pooch>=1.5 (from mne->mne-features)\n",
      "  Downloading pooch-1.8.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.11/site-packages (from mne->mne-features) (5.1.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from mne->mne-features) (23.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from mne->mne-features) (3.1.3)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from mne->mne-features) (0.3)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/anaconda3/lib/python3.11/site-packages (from numba->mne-features) (0.42.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->mne-features) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->mne-features) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->mne-features) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn->mne-features) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn->mne-features) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne->mne-features) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne->mne-features) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne->mne-features) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne->mne-features) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne->mne-features) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne->mne-features) (3.0.9)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from pooch>=1.5->mne->mne-features) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from pooch>=1.5->mne->mne-features) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->mne-features) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->mne->mne-features) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne->mne-features) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne->mne-features) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne->mne-features) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne->mne-features) (2024.2.2)\n",
      "Downloading mne_features-0.3-py3-none-any.whl (26 kB)\n",
      "Downloading mne-1.6.1-py3-none-any.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pooch, mne, mne-features\n",
      "Successfully installed mne-1.6.1 mne-features-0.3 pooch-1.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mne-features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne_features.univariate as mne_f\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def time_series_features(data):\n",
    "\n",
    "    n_trials, n_secs, n_channels, _ = data.shape\n",
    "    features_per_channel = 3\n",
    "\n",
    "    features = np.empty([n_trials, n_secs, n_channels * features_per_channel])\n",
    "    for i, trial in enumerate(data):\n",
    "        for j, second in enumerate(trial):\n",
    "            variance = mne_f.compute_variance(second)\n",
    "            rms = mne_f.compute_rms(second)\n",
    "            ptp_amp = mne_f.compute_ptp_amp(second)\n",
    "            features[i][j] = np.concatenate([variance, rms, ptp_amp])\n",
    "    features = features.reshape(\n",
    "        [n_trials*n_secs, n_channels*features_per_channel])\n",
    "    return features\n",
    "\n",
    "# features_list = time_series_features(data)\n",
    "\n",
    "# # Print the features list\n",
    "# print(features_list)\n",
    "\n",
    "\n",
    "\n",
    "def freq_band_features(data, freq_bands):\n",
    "    \n",
    "  \n",
    "    n_trials, n_secs, n_channels, sfreq = data.shape\n",
    "    features_per_channel = len(freq_bands)-1\n",
    "\n",
    "    features = np.empty([n_trials, n_secs, n_channels * features_per_channel])\n",
    "    for i, trial in enumerate(data):\n",
    "        for j, second in enumerate(trial):\n",
    "            psd = mne_f.compute_pow_freq_bands(\n",
    "                sfreq, second, freq_bands=freq_bands)\n",
    "            features[i][j] = psd\n",
    "    features = features.reshape(\n",
    "        [n_trials*n_secs, n_channels*features_per_channel])\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def hjorth_features(data):\n",
    "  \n",
    "    n_trials, n_secs, n_channels, sfreq = data.shape\n",
    "    features_per_channel = 2\n",
    "\n",
    "    features = np.empty([n_trials, n_secs, n_channels * features_per_channel])\n",
    "    for i, trial in enumerate(data):\n",
    "        for j, second in enumerate(trial):\n",
    "            mobility_spect = mne_f.compute_hjorth_mobility_spect(sfreq, second)\n",
    "            complexity_spect = mne_f.compute_hjorth_complexity_spect(\n",
    "                sfreq, second)\n",
    "            features[i][j] = np.concatenate([mobility_spect, complexity_spect])\n",
    "    features = features.reshape(\n",
    "        [n_trials*n_secs, n_channels*features_per_channel])\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def fractal_features(data):\n",
    " \n",
    "    n_trials, n_secs, n_channels, _ = data.shape\n",
    "    features_per_channel = 2\n",
    "\n",
    "    features = np.empty([n_trials, n_secs, n_channels * features_per_channel])\n",
    "    for i, trial in enumerate(data):\n",
    "        for j, second in enumerate(trial):\n",
    "            higuchi = mne_f.compute_higuchi_fd(second)\n",
    "            katz = mne_f.compute_katz_fd(second)\n",
    "            features[i][j] = np.concatenate([higuchi, katz])\n",
    "    features = features.reshape(\n",
    "        [n_trials*n_secs, n_channels*features_per_channel])\n",
    "    return features\n",
    "\n",
    "\n",
    "def entropy_features(data):\n",
    "\n",
    "    n_trials, n_secs, n_channels, sfreq = data.shape\n",
    "    features_per_channel = 4\n",
    "\n",
    "    features = np.empty([n_trials, n_secs, n_channels * features_per_channel])\n",
    "    for i, trial in enumerate(data):\n",
    "        for j, second in enumerate(trial):\n",
    "            app_entropy = mne_f.compute_app_entropy(second)\n",
    "            samp_entropy = mne_f.compute_samp_entropy(second)\n",
    "            spect_entropy = mne_f.compute_spect_entropy(sfreq, second)\n",
    "            svd_entropy = mne_f.compute_svd_entropy(second)\n",
    "            features[i][j] = np.concatenate(\n",
    "                [app_entropy, samp_entropy, spect_entropy, svd_entropy])\n",
    "    features = features.reshape(\n",
    "        [n_trials*n_secs, n_channels*features_per_channel])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-macos==2.15.0 (from tensorflow)\n",
      "  Downloading tensorflow_macos-2.15.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl.metadata (849 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.9.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.4)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.41.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.15.0-cp311-cp311-macosx_12_0_arm64.whl (2.1 kB)\n",
      "Downloading tensorflow_macos-2.15.0-cp311-cp311-macosx_12_0_arm64.whl (208.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.8/208.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl (20.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.9/186.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.7 gast-0.5.4 google-auth-2.28.1 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.62.1 keras-2.15.0 libclang-16.0.6 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 tensorflow-macos-2.15.0 termcolor-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.11/site-packages (from keras-tuner) (2.15.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from keras-tuner) (23.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from keras-tuner) (2.31.0)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->keras-tuner) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->keras-tuner) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->keras-tuner) (2024.2.2)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlrd\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xlrd\n",
      "Successfully installed xlrd-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ... False False False]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras import models, Input\n",
    "from keras import optimizers as opt\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import variables as v\n",
    "\n",
    "# from dataset import load_dataset, load_labels, split_data, format_labels\n",
    "# from features import time_series_features, fractal_features, entropy_features, hjorth_features, freq_band_features\n",
    "\n",
    "data_type = \"ica_filtered\"\n",
    "test_type = \"Arithmetic\"\n",
    "\n",
    "dataset_ = load_dataset(data_type=data_type, test_type=test_type)\n",
    "dataset = split_data(dataset_, v.SFREQ)\n",
    "\n",
    "label_ = load_labels()\n",
    "label = format_labels(label_, test_type=test_type, epochs=dataset.shape[1])\n",
    "print(label)\n",
    "\n",
    "\n",
    "# features = time_series_features(dataset)\n",
    "# freq_bands = np.array([1, 4, 8, 12, 30, 50])\n",
    "# features = freq_band_features(dataset, freq_bands)\n",
    "# features = hjorth_features(dataset)\n",
    "# features = entropy_features(dataset)\n",
    "# features = fractal_features(dataset)\n",
    "\n",
    "# features1 = time_series_features(dataset)\n",
    "# freq_bands = np.array([1, 4, 8, 12, 30, 50])\n",
    "# features2 = freq_band_features(dataset, freq_bands)\n",
    "# features3 = hjorth_features(dataset)\n",
    "# features4 = entropy_features(dataset)\n",
    "# features5 = fractal_features(dataset)\n",
    "\n",
    "# Feature Fusion\n",
    "# concatenated_features = np.concatenate(( features1, features3), axis=1)\n",
    "# data = features4;\n",
    "# c = concatenated_features;\n",
    "\n",
    "# data = features\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction method: time_series_features\n",
      "KNN Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.86      0.88       475\n",
      "        True       0.85      0.91      0.88       425\n",
      "\n",
      "    accuracy                           0.88       900\n",
      "   macro avg       0.88      0.88      0.88       900\n",
      "weighted avg       0.88      0.88      0.88       900\n",
      "\n",
      "KNN Accuracy: 0.8811111111111111\n",
      "SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.87      0.86       475\n",
      "        True       0.85      0.84      0.84       425\n",
      "\n",
      "    accuracy                           0.85       900\n",
      "   macro avg       0.85      0.85      0.85       900\n",
      "weighted avg       0.85      0.85      0.85       900\n",
      "\n",
      "SVM Accuracy: 0.8533333333333334\n",
      "Ensemble Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.89      0.90       475\n",
      "        True       0.88      0.92      0.90       425\n",
      "\n",
      "    accuracy                           0.90       900\n",
      "   macro avg       0.90      0.90      0.90       900\n",
      "weighted avg       0.90      0.90      0.90       900\n",
      "\n",
      "Ensemble Accuracy: 0.9011111111111111\n",
      "\n",
      "\n",
      "\n",
      "Feature extraction method: hjorth_features\n",
      "KNN Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.97      0.97       475\n",
      "        True       0.97      0.97      0.97       425\n",
      "\n",
      "    accuracy                           0.97       900\n",
      "   macro avg       0.97      0.97      0.97       900\n",
      "weighted avg       0.97      0.97      0.97       900\n",
      "\n",
      "KNN Accuracy: 0.9711111111111111\n",
      "SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.96      0.95       475\n",
      "        True       0.95      0.94      0.95       425\n",
      "\n",
      "    accuracy                           0.95       900\n",
      "   macro avg       0.95      0.95      0.95       900\n",
      "weighted avg       0.95      0.95      0.95       900\n",
      "\n",
      "SVM Accuracy: 0.95\n",
      "Ensemble Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97       475\n",
      "        True       0.96      0.97      0.97       425\n",
      "\n",
      "    accuracy                           0.97       900\n",
      "   macro avg       0.97      0.97      0.97       900\n",
      "weighted avg       0.97      0.97      0.97       900\n",
      "\n",
      "Ensemble Accuracy: 0.9688888888888889\n",
      "\n",
      "\n",
      "\n",
      "Feature extraction method: entropy_features\n",
      "KNN Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.61      0.61       475\n",
      "        True       0.56      0.56      0.56       425\n",
      "\n",
      "    accuracy                           0.58       900\n",
      "   macro avg       0.58      0.58      0.58       900\n",
      "weighted avg       0.58      0.58      0.58       900\n",
      "\n",
      "KNN Accuracy: 0.5844444444444444\n",
      "SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.62      0.59      0.60       475\n",
      "        True       0.57      0.59      0.58       425\n",
      "\n",
      "    accuracy                           0.59       900\n",
      "   macro avg       0.59      0.59      0.59       900\n",
      "weighted avg       0.59      0.59      0.59       900\n",
      "\n",
      "SVM Accuracy: 0.5922222222222222\n",
      "Ensemble Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.63      0.63      0.63       475\n",
      "        True       0.58      0.58      0.58       425\n",
      "\n",
      "    accuracy                           0.60       900\n",
      "   macro avg       0.60      0.60      0.60       900\n",
      "weighted avg       0.60      0.60      0.60       900\n",
      "\n",
      "Ensemble Accuracy: 0.6044444444444445\n",
      "\n",
      "\n",
      "\n",
      "Feature extraction method: fractal_features\n",
      "KNN Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.61      0.59       475\n",
      "        True       0.53      0.50      0.51       425\n",
      "\n",
      "    accuracy                           0.56       900\n",
      "   macro avg       0.56      0.55      0.55       900\n",
      "weighted avg       0.56      0.56      0.56       900\n",
      "\n",
      "KNN Accuracy: 0.5577777777777778\n",
      "SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.59      0.64      0.61       475\n",
      "        True       0.55      0.50      0.53       425\n",
      "\n",
      "    accuracy                           0.57       900\n",
      "   macro avg       0.57      0.57      0.57       900\n",
      "weighted avg       0.57      0.57      0.57       900\n",
      "\n",
      "SVM Accuracy: 0.5733333333333334\n",
      "Ensemble Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.54      0.56       475\n",
      "        True       0.52      0.56      0.54       425\n",
      "\n",
      "    accuracy                           0.55       900\n",
      "   macro avg       0.55      0.55      0.55       900\n",
      "weighted avg       0.55      0.55      0.55       900\n",
      "\n",
      "Ensemble Accuracy: 0.5511111111111111\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most Optimized Models till now \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the list of feature extraction methods\n",
    "feature_extraction_methods = [\n",
    "    time_series_features,\n",
    "    hjorth_features,\n",
    "    entropy_features,\n",
    "    fractal_features\n",
    "]\n",
    "\n",
    "# Iterate over each feature extraction method\n",
    "for feature_extraction_method in feature_extraction_methods:\n",
    "    print(\"Feature extraction method:\", feature_extraction_method.__name__)\n",
    "\n",
    "    # Extract features using the current method\n",
    "    features = feature_extraction_method(dataset)\n",
    "\n",
    "    # Splitting data into training, validation, and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, label, test_size=0.3, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_val = scaler.transform(x_val)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    knn_pipeline = Pipeline([\n",
    "        ('pca', PCA(n_components=50, random_state=42)),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "    knn_param_grid = [{\n",
    "        'pca__n_components': [10, 25, 50],\n",
    "        'knn__n_neighbors': [3, 5, 7, 10],\n",
    "        'knn__weights': ['uniform', 'distance'],\n",
    "        'knn__p': [1, 2]\n",
    "    }]\n",
    "    knn_clf = GridSearchCV(knn_pipeline, knn_param_grid, cv=5)\n",
    "    knn_clf.fit(x_train, y_train)\n",
    "\n",
    "    svm_pipeline = Pipeline([\n",
    "        ('pca', PCA(n_components=50, random_state=42)),\n",
    "        ('svm', SVC(kernel='rbf', probability=True))\n",
    "    ])\n",
    "    svm_param_grid = [{\n",
    "        'pca__n_components': [10, 25, 50],\n",
    "        'svm__C': [0.1, 1, 10, 100],\n",
    "        'svm__gamma': ['scale', 'auto']\n",
    "    }]\n",
    "    svm_clf = GridSearchCV(svm_pipeline, svm_param_grid, cv=5)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "\n",
    "    # Ensemble of KNN and SVM models with weight 0.5\n",
    "    ensemble_clf = VotingClassifier(estimators=[('knn', knn_clf), ('svm', svm_clf)], voting='soft', weights=[0.5, 0.5])\n",
    "    ensemble_clf.fit(x_train, y_train)\n",
    "\n",
    "    # Random forest classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf_clf.fit(x_train, y_train)\n",
    "\n",
    "    knn_pred_test = knn_clf.predict(x_test)\n",
    "    svm_pred_test = svm_clf.predict(x_test)\n",
    "    ensemble_pred_test = ensemble_clf.predict(x_test)\n",
    "    rf_pred_test = rf_clf.predict(x_test)\n",
    "\n",
    "    print('KNN Results:')\n",
    "    print(classification_report(y_test, knn_pred_test))\n",
    "    print('KNN Accuracy:', accuracy_score(y_test, knn_pred_test))\n",
    "    print('SVM Results:')\n",
    "    print(classification_report(y_test, svm_pred_test))\n",
    "    print('SVM Accuracy:', accuracy_score(y_test, svm_pred_test))\n",
    "    print('Ensemble Results:')\n",
    "    print(classification_report(y_test, ensemble_pred_test))\n",
    "    print('Ensemble Accuracy:', accuracy_score(y_test, ensemble_pred_test))\n",
    "    # print('Random Forest Results:')\n",
    "    # print(classification_report(y_test, rf_pred_test))\n",
    "    # print('Random Forest Accuracy:', accuracy_score(y_test, rf_pred_test))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.97      0.97       475\n",
      "        True       0.97      0.97      0.97       425\n",
      "\n",
      "    accuracy                           0.97       900\n",
      "   macro avg       0.97      0.97      0.97       900\n",
      "weighted avg       0.97      0.97      0.97       900\n",
      "\n",
      "KNN Accuracy: 0.9711111111111111\n",
      "SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.96      0.95       475\n",
      "        True       0.95      0.94      0.95       425\n",
      "\n",
      "    accuracy                           0.95       900\n",
      "   macro avg       0.95      0.95      0.95       900\n",
      "weighted avg       0.95      0.95      0.95       900\n",
      "\n",
      "SVM Accuracy: 0.95\n",
      "Ensemble Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97       475\n",
      "        True       0.96      0.97      0.97       425\n",
      "\n",
      "    accuracy                           0.97       900\n",
      "   macro avg       0.97      0.97      0.97       900\n",
      "weighted avg       0.97      0.97      0.97       900\n",
      "\n",
      "Ensemble Accuracy: 0.9688888888888889\n",
      "Random Forest Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.81      0.84       475\n",
      "        True       0.80      0.88      0.84       425\n",
      "\n",
      "    accuracy                           0.84       900\n",
      "   macro avg       0.84      0.84      0.84       900\n",
      "weighted avg       0.84      0.84      0.84       900\n",
      "\n",
      "Random Forest Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Splitting data into training, validation, and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.3, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Pipeline for KNN model\n",
    "knn_pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=50, random_state=42)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "# Hyperparameter tuning for KNN model\n",
    "knn_param_grid = [{\n",
    "    'pca__n_components': [10, 25, 50],\n",
    "    'knn__n_neighbors': [3, 5, 7, 10],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2]\n",
    "}]\n",
    "knn_clf = GridSearchCV(knn_pipeline, knn_param_grid, cv=5)\n",
    "knn_clf.fit(x_train, y_train)\n",
    "\n",
    "# Pipeline for SVM model\n",
    "svm_pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=50, random_state=42)),\n",
    "    ('svm', SVC(kernel='rbf', probability=True))\n",
    "])\n",
    "# Hyperparameter tuning for SVM model\n",
    "svm_param_grid = [{\n",
    "    'pca__n_components': [10, 25, 50],\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}]\n",
    "svm_clf = GridSearchCV(svm_pipeline, svm_param_grid, cv=5)\n",
    "svm_clf.fit(x_train, y_train)\n",
    "\n",
    "# Ensemble of KNN and SVM models with weight 0.5\n",
    "ensemble_clf = VotingClassifier(estimators=[('knn', knn_clf), ('svm', svm_clf)], voting='soft', weights=[0.5, 0.5])\n",
    "ensemble_clf.fit(x_train, y_train)\n",
    "\n",
    "# Random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_clf.fit(x_train, y_train)\n",
    "\n",
    "# Making predictions on test set\n",
    "knn_pred_test = knn_clf.predict(x_test)\n",
    "svm_pred_test = svm_clf.predict(x_test)\n",
    "ensemble_pred_test = ensemble_clf.predict(x_test)\n",
    "rf_pred_test = rf_clf.predict(x_test)\n",
    "\n",
    "# Evaluating models\n",
    "print('KNN Results:')\n",
    "print(classification_report(y_test, knn_pred_test))\n",
    "print('KNN Accuracy:', accuracy_score(y_test, knn_pred_test))\n",
    "print('SVM Results:')\n",
    "print(classification_report(y_test, svm_pred_test))\n",
    "print('SVM Accuracy:', accuracy_score(y_test, svm_pred_test))\n",
    "print('Ensemble Results:')\n",
    "print(classification_report(y_test, ensemble_pred_test))\n",
    "print('Ensemble Accuracy:', accuracy_score(y_test, ensemble_pred_test))\n",
    "print('Random Forest Results:')\n",
    "print(classification_report(y_test, rf_pred_test))\n",
    "print('Random Forest Accuracy:', accuracy_score(y_test, rf_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test, knn_pred_test, svm_pred_test, ensemble_pred_test, rf_pred_test are defined from model predictions\n",
    "\n",
    "# List of model predictions and labels for plotting\n",
    "model_predictions = [\n",
    "    (knn_pred_test, \"KNN\"),\n",
    "    (svm_pred_test, \"SVM\"),\n",
    "    # (ensemble_pred_test, \"Ensemble\"),\n",
    "    # Uncomment the next line if Random Forest predictions are available\n",
    "    # (rf_pred_test, \"Random Forest\")\n",
    "]\n",
    "\n",
    "# Define class names if they are not straightforward (replace 'class_labels' with actual names if necessary)\n",
    "class_names = ['Class 0', 'Class 1', 'Class 2']  # Adjust according to your classification labels\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, model_name, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(f'{title} for {model_name}')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Loop through all models and plot their confusion matrices\n",
    "for predictions, model_name in model_predictions:\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    plot_confusion_matrix(cm, model_name, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Use GridSearchCV to find the best hyperparameters for knn\u001b[39;00m\n\u001b[1;32m     37\u001b[0m knn_param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleaf_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m),\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     41\u001b[0m }\n\u001b[0;32m---> 42\u001b[0m split_index \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_train)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train_val))]\n\u001b[1;32m     43\u001b[0m ps \u001b[38;5;241m=\u001b[39m PredefinedSplit(test_fold\u001b[38;5;241m=\u001b[39msplit_index)\n\u001b[1;32m     44\u001b[0m knn_clf \u001b[38;5;241m=\u001b[39m GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv\u001b[38;5;241m=\u001b[39mps, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_val' is not defined"
     ]
    }
   ],
   "source": [
    "# AKSHAT'S CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Load the sam40 dataset\n",
    "# data = np.load(\"sam40.npz\")\n",
    "# X = data['X']\n",
    "# y = data['y']\n",
    "\n",
    "# # Split dataset into training, validation, and test sets\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=1)\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(\n",
    "    data, label, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "# Scale the training, validation, and test sets\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "X_train_scaled = scaler.transform(x_train)\n",
    "X_val_scaled = scaler.transform(x_val)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters for knn\n",
    "knn_param_grid = {\n",
    "    'leaf_size': range(50),\n",
    "    'n_neighbors': range(1, 10),\n",
    "    'p': [1, 2]\n",
    "}\n",
    "split_index = [-1 if x in range(len(x_train)) else 0 for x in range(len(X_train_val))]\n",
    "ps = PredefinedSplit(test_fold=split_index)\n",
    "knn_clf = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=ps, refit=True)\n",
    "knn_clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters for svm\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1]\n",
    "}\n",
    "svm_clf = GridSearchCV(SVC(kernel='rbf'), svm_param_grid, cv=5, refit=True)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation set using both knn and svm models\n",
    "knn_y_val = knn_clf.predict(X_val)\n",
    "svm_y_val = svm_clf.predict(X_val_scaled)\n",
    "\n",
    "# Create a voting classifier that takes the majority vote of both knn and svm models\n",
    "voting_clf = VotingClassifier(estimators=[('knn', knn_clf), ('svm', svm_clf)], voting='hard')\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the combined model's performance on the test set\n",
    "knn_y_test = knn_clf.predict(X_test)\n",
    "svm_y_test = svm_clf.predict(X_test_scaled)\n",
    "voting_y_test = voting_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"KNN accuracy:\", accuracy_score(y_test, knn_y_test))\n",
    "print(\"SVM accuracy:\", accuracy_score(y_test, svm_y_test))\n",
    "print(\"Voting accuracy:\", accuracy_score(y_test, voting_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "18 fits failed out of a total of 900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 213, in fit\n",
      "    self._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'leaf_size' parameter of KNeighborsClassifier must be an int in the range [1, inf). Got 0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667\n",
      " 0.95       0.94833333 0.94       0.94166667 0.94333333 0.95333333\n",
      " 0.93833333 0.94166667 0.94       0.93666667 0.93833333 0.93\n",
      " 0.92666667 0.92       0.92166667 0.91666667 0.915      0.90666667]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.94      0.96       311\n",
      "        True       0.94      0.97      0.95       289\n",
      "\n",
      "    accuracy                           0.95       600\n",
      "   macro avg       0.96      0.96      0.95       600\n",
      "weighted avg       0.96      0.95      0.96       600\n",
      "\n",
      "[[292  19]\n",
      " [  8 281]]\n",
      "Accuracy: 0.955\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predict_and_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_true, y_pred)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n\u001b[0;32m---> 54\u001b[0m predict_and_plot(knn_clf, x_val, y_val)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_and_plot' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras import models, Input\n",
    "from keras import optimizers as opt\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# KNN\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(\n",
    "    data, label, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=1)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x = scaler.transform(x)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "param_grid = {\n",
    "    'leaf_size': range(50),\n",
    "    'n_neighbors': range(1, 10),\n",
    "    'p': [1, 2]\n",
    "}\n",
    "split_index = [-1 if x in range(len(x_train)) else 0 for x in range(len(x))]\n",
    "ps = PredefinedSplit(test_fold=split_index)\n",
    "knn_clf = GridSearchCV(KNeighborsClassifier(), param_grid, cv=ps, refit=True)\n",
    "knn_clf.fit(x, y)\n",
    "\n",
    "\n",
    "y_pred = knn_clf.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred))\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "predict_and_plot(knn_clf, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.94      0.94       311\n",
      "        True       0.94      0.93      0.93       289\n",
      "\n",
      "    accuracy                           0.94       600\n",
      "   macro avg       0.94      0.94      0.94       600\n",
      "weighted avg       0.94      0.94      0.94       600\n",
      "\n",
      "[[293  18]\n",
      " [ 20 269]]\n",
      "Accuracy: 0.9366666666666666\n"
     ]
    }
   ],
   "source": [
    "## SVM\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(\n",
    "    data, label, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "split_index = [-1 if x in range(len(x_train)) else 0 for x in range(len(x))]\n",
    "ps = PredefinedSplit(test_fold=split_index)\n",
    "svm_clf = GridSearchCV(SVC(), param_grid, cv=ps, refit=True)\n",
    "svm_clf.fit(x, y)\n",
    "\n",
    "y_pred = svm_clf.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred))\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# predict_and_plot(svm_clf, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "18 fits failed out of a total of 900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 213, in fit\n",
      "    self._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'leaf_size' parameter of KNeighborsClassifier must be an int in the range [1, inf). Got 0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935\n",
      " 0.95833333 0.95666667 0.935      0.94       0.945      0.94833333\n",
      " 0.94       0.94166667 0.945      0.93666667 0.94666667 0.94\n",
      " 0.93333333 0.93666667 0.92833333 0.94333333 0.92166667 0.935     ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.98      0.95       311\n",
      "        True       0.98      0.92      0.95       289\n",
      "\n",
      "    accuracy                           0.95       600\n",
      "   macro avg       0.95      0.95      0.95       600\n",
      "weighted avg       0.95      0.95      0.95       600\n",
      "\n",
      "[[306   5]\n",
      " [ 24 265]]\n",
      "Accuracy: 0.9516666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Assuming knn_clf and svm_clf are already trained models\n",
    "\n",
    "# Create a Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('knn', knn_clf),\n",
    "    ('svm', svm_clf)\n",
    "], voting='hard')  # 'hard' voting means majority class wins\n",
    "\n",
    "# Train the ensemble model\n",
    "voting_clf.fit(x, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_ensemble = voting_clf.predict(x_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "print(\"Ensemble Model:\")\n",
    "print(metrics.classification_report(y_true, y_pred_ensemble))\n",
    "print(metrics.confusion_matrix(y_true, y_pred_ensemble))\n",
    "\n",
    "accuracy_ensemble = accuracy_score(y_true, y_pred_ensemble)\n",
    "print(\"Accuracy:\", accuracy_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.94      0.95       311\n",
      "        True       0.94      0.95      0.94       289\n",
      "\n",
      "    accuracy                           0.94       600\n",
      "   macro avg       0.94      0.95      0.94       600\n",
      "weighted avg       0.95      0.94      0.95       600\n",
      "\n",
      "[[293  18]\n",
      " [ 15 274]]\n",
      "Accuracy: 0.945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "import numpy as np\n",
    "\n",
    "# Assuming data and label are your features and labels\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "x, x_test, y, y_test = train_test_split(data, label, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Train KNN model\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Get KNN predictions for training and validation sets\n",
    "knn_train_predictions = knn_clf.predict(x_train_scaled)\n",
    "knn_val_predictions = knn_clf.predict(x_val_scaled)\n",
    "\n",
    "# Add KNN predictions as features for training the SVM model\n",
    "x_train_combined = np.column_stack((x_train_scaled, knn_train_predictions))\n",
    "x_val_combined = np.column_stack((x_val_scaled, knn_val_predictions))\n",
    "\n",
    "# Train SVM model\n",
    "param_grid_svm = {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['rbf']}\n",
    "split_index_svm = [-1] * len(x_train_combined) + [0] * len(x_val_combined)\n",
    "ps_svm = PredefinedSplit(test_fold=split_index_svm)\n",
    "svm_clf = GridSearchCV(SVC(), param_grid_svm, cv=ps_svm, refit=True)\n",
    "svm_clf.fit(np.vstack((x_train_combined, x_val_combined)), np.concatenate((y_train, y_val)))\n",
    "\n",
    "# Get SVM predictions on the test set\n",
    "svm_test_predictions = svm_clf.predict(np.column_stack((x_test_scaled, knn_clf.predict(x_test_scaled))))\n",
    "\n",
    "# Evaluate the SVM model\n",
    "print(\"SVM Model:\")\n",
    "print(classification_report(y_test, svm_test_predictions))\n",
    "print(confusion_matrix(y_test, svm_test_predictions))\n",
    "accuracy_svm = accuracy_score(y_test, svm_test_predictions)\n",
    "print(\"Accuracy:\", accuracy_svm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.79      0.77       311\n",
      "        True       0.76      0.70      0.73       289\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "[[247  64]\n",
      " [ 86 203]]\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x, x_test, y, y_test = train_test_split(\n",
    "    data, label, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "# Define hyperparameter grid for Decision Tree\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define PredefinedSplit\n",
    "split_index = [-1 if x in range(len(x_train)) else 0 for x in range(len(x))]\n",
    "ps = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "# Create GridSearchCV instance for Decision Tree\n",
    "dt_clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=ps, refit=True)\n",
    "\n",
    "# Train the Decision Tree classifier\n",
    "dt_clf.fit(x, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_clf.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(metrics.classification_report(y_true, y_pred))\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m rf_clf \u001b[38;5;241m=\u001b[39m GridSearchCV(RandomForestClassifier(), param_grid, cv\u001b[38;5;241m=\u001b[39mps, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Train the Random Forest classifier\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m rf_clf\u001b[38;5;241m.\u001b[39mfit(x, y)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m     29\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rf_clf\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    730\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 732\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x, x_test, y, y_test = train_test_split(\n",
    "    data, label, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Define PredefinedSplit\n",
    "split_index = [-1 if x in range(len(x_train)) else 0 for x in range(len(x))]\n",
    "ps = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "# Create GridSearchCV instance for Random Forest\n",
    "rf_clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=ps, refit=True)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf_clf.fit(x, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_clf.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(metrics.classification_report(y_true, y_pred))\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Code - Added other models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.83%\n",
      "F1 Score: 0.62\n",
      "Accuracy: 57.00%\n",
      "F1 Score: 0.36\n",
      "Accuracy: 58.33%\n",
      "F1 Score: 0.38\n",
      "Accuracy: 56.83%\n",
      "F1 Score: 0.60\n",
      "Accuracy: 56.50%\n",
      "F1 Score: 0.59\n",
      "Accuracy: 62.17%\n",
      "F1 Score: 0.56\n",
      "Accuracy: 63.17%\n",
      "F1 Score: 0.62\n",
      "Accuracy: 62.33%\n",
      "F1 Score: 0.61\n",
      "Accuracy: 63.17%\n",
      "F1 Score: 0.63\n",
      "Test Set Accuracy: 60.83%\n",
      "Validation Set Accuracy: 63.17%\n",
      "Accuracy: 63.17%\n",
      "F1 Score: 0.63\n",
      "0.6996336996336996\n",
      "0.5787878787878787\n",
      "AUC Score:0.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True,  True, False, False, False, False,\n",
       "        True,  True,  True,  True, False, False, False,  True, False,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True, False,  True, False,  True, False,  True, False,  True,\n",
       "       False, False,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False, False, False,  True,  True,  True,\n",
       "       False,  True,  True, False, False,  True, False, False,  True,\n",
       "       False,  True,  True, False,  True, False,  True, False,  True,\n",
       "       False,  True, False,  True, False,  True,  True,  True, False,\n",
       "        True,  True,  True, False, False,  True,  True,  True, False,\n",
       "       False, False, False, False,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True, False, False,  True, False, False,\n",
       "       False,  True, False,  True,  True,  True,  True, False, False,\n",
       "       False,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "       False,  True, False,  True,  True, False,  True,  True,  True,\n",
       "       False, False, False,  True,  True,  True, False, False,  True,\n",
       "        True,  True, False, False, False,  True,  True,  True, False,\n",
       "        True, False,  True, False, False, False, False,  True,  True,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "        True,  True,  True, False,  True,  True,  True,  True, False,\n",
       "        True,  True, False, False, False,  True,  True, False,  True,\n",
       "        True, False,  True, False, False, False,  True,  True, False,\n",
       "        True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False, False, False,  True, False,  True,  True,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "       False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False, False,  True,  True,  True, False,  True,  True,  True,\n",
       "       False, False,  True, False,  True, False, False, False,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False, False, False,  True, False, False,  True, False,  True,\n",
       "        True, False, False,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True, False,  True, False, False, False,\n",
       "        True,  True,  True,  True,  True, False,  True, False, False,\n",
       "       False, False, False,  True,  True, False,  True,  True, False,\n",
       "       False, False,  True, False,  True,  True,  True,  True, False,\n",
       "        True,  True, False, False, False, False,  True, False,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "       False,  True, False,  True,  True,  True, False,  True, False,\n",
       "        True, False, False, False, False, False, False,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True, False,  True,\n",
       "       False, False,  True,  True, False, False, False,  True, False,\n",
       "       False,  True, False, False,  True, False,  True,  True,  True,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "        True, False, False,  True,  True, False,  True,  True,  True,\n",
       "        True,  True, False,  True, False,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False, False, False, False,  True, False, False,\n",
       "        True, False,  True,  True,  True,  True, False, False,  True,\n",
       "        True, False,  True,  True,  True, False,  True,  True, False,\n",
       "        True, False, False,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True, False, False, False,  True,  True, False,\n",
       "       False,  True, False, False, False, False, False,  True,  True,\n",
       "        True,  True,  True, False, False, False,  True, False,  True,\n",
       "       False, False, False, False, False,  True,  True, False,  True,\n",
       "        True, False, False, False, False,  True,  True, False,  True,\n",
       "        True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "       False,  True, False, False,  True, False, False,  True, False,\n",
       "       False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False, False, False, False, False,  True,  True,  True,  True,\n",
       "       False,  True,  True, False, False,  True,  True,  True, False,\n",
       "        True, False, False,  True, False, False,  True,  True,  True,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "        True, False, False, False,  True, False,  True, False, False,\n",
       "       False, False,  True, False, False,  True,  True,  True, False,\n",
       "        True,  True, False,  True,  True,  True, False, False, False,\n",
       "       False, False, False,  True,  True,  True])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "######## DECISION TREE\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(\n",
    "    data, label, test_size=0.2, random_state=1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "acc=[]\n",
    "f1_list=[]\n",
    "for i in range(1,10) :\n",
    "    dt_clf = DecisionTreeClassifier(random_state=1, max_depth = i)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "    dt_clf.fit(x_train, y_train)\n",
    "    \n",
    "    preds = dt_clf.predict(x_val)\n",
    "    probs = dt_clf.predict_proba(x_val)\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "    acc.append(accuracy)\n",
    "    f1 = f1_score(y_val,preds)\n",
    "    f1_list.append(f1)\n",
    "    print(\"F1 Score: {:.2f}\".format(f1))\n",
    "\n",
    "\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = dt_clf.predict(x_val)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = dt_clf.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = dt_clf.predict(x_val)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = dt_clf.predict(x_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Set Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate accuracy on the validation set\n",
    "val_accuracy = metrics.accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Set Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "predict_and_plot(dt_clf, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.50%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m     16\u001b[0m acc\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[0;32m---> 17\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_val,preds)\n\u001b[1;32m     18\u001b[0m f1_list\u001b[38;5;241m.\u001b[39mappend(f1)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f1))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "########### KNN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initialize KNN Classifier\n",
    "acc=[]\n",
    "f1_list=[]\n",
    "for i in range(1,5) :\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_clf.fit(x_train, y_train)\n",
    "    \n",
    "    preds = knn_clf.predict(x_val)\n",
    "#     probs = dt_clf.predict_proba(x_val)\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "    acc.append(accuracy)\n",
    "    f1 = f1_score(y_val,preds)\n",
    "    f1_list.append(f1)\n",
    "    print(\"F1 Score: {:.2f}\".format(f1))\n",
    "\n",
    "plt.plot(f1_list)\n",
    "plt.plot(acc) \n",
    "\n",
    "predict_and_plot(knn_clf, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(model, inputs, targets, name=''):\n",
    "    preds = model.predict(inputs)\n",
    "    probs = model.predict_proba(inputs)\n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "    f1 = f1_score(targets,preds)\n",
    "    print(\"F1 Score: {:.2f}\".format(f1))\n",
    "    recall = recall_score(targets,preds)\n",
    "    print(recall)\n",
    "    precision = precision_score(targets,preds)\n",
    "    print(precision)\n",
    "    auc = roc_auc_score(targets,preds)\n",
    "    print(\"AUC Score:{:.2f}\".format(auc))\n",
    "    cf = confusion_matrix(targets, preds)\n",
    "    plt.figure()\n",
    "    sns.heatmap(cf, annot=True)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title('{} Confusion Matrix'.format(name));\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (n_estimators=1): 56.17%\n",
      "F1 Score (n_estimators=1): 0.52\n",
      "Accuracy (n_estimators=2): 56.83%\n",
      "F1 Score (n_estimators=2): 0.37\n",
      "Accuracy (n_estimators=3): 59.33%\n",
      "F1 Score (n_estimators=3): 0.55\n",
      "Accuracy (n_estimators=4): 59.33%\n",
      "F1 Score (n_estimators=4): 0.46\n",
      "Accuracy (n_estimators=5): 61.50%\n",
      "F1 Score (n_estimators=5): 0.59\n",
      "Accuracy (n_estimators=6): 62.50%\n",
      "F1 Score (n_estimators=6): 0.52\n",
      "Accuracy (n_estimators=7): 63.83%\n",
      "F1 Score (n_estimators=7): 0.61\n",
      "Accuracy (n_estimators=8): 64.67%\n",
      "F1 Score (n_estimators=8): 0.57\n",
      "Accuracy (n_estimators=9): 65.83%\n",
      "F1 Score (n_estimators=9): 0.63\n",
      "Accuracy: 65.83%\n",
      "F1 Score: 0.63\n",
      "0.6336996336996337\n",
      "0.6223021582733813\n",
      "AUC Score:0.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True,  True, False, False, False, False,\n",
       "        True, False, False, False,  True, False, False,  True, False,\n",
       "        True,  True,  True, False,  True, False, False,  True,  True,\n",
       "        True, False,  True, False,  True, False,  True, False, False,\n",
       "       False, False, False,  True, False,  True,  True, False,  True,\n",
       "       False,  True, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False,  True,  True,  True, False,  True,\n",
       "       False,  True, False,  True,  True,  True, False,  True, False,\n",
       "       False,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True,  True, False, False,  True, False, False, False, False,\n",
       "        True, False, False, False,  True,  True,  True,  True,  True,\n",
       "       False, False,  True,  True,  True, False, False,  True, False,\n",
       "        True,  True, False, False,  True,  True, False, False,  True,\n",
       "       False,  True, False,  True, False, False, False,  True, False,\n",
       "       False,  True, False,  True,  True,  True, False, False, False,\n",
       "        True, False, False, False,  True,  True, False,  True,  True,\n",
       "        True, False,  True, False, False,  True, False,  True, False,\n",
       "        True, False, False,  True, False, False,  True, False,  True,\n",
       "        True, False, False, False, False,  True, False, False, False,\n",
       "        True,  True,  True, False, False,  True,  True,  True, False,\n",
       "        True,  True, False, False, False,  True, False,  True,  True,\n",
       "       False, False,  True, False,  True, False, False,  True, False,\n",
       "       False, False, False,  True,  True, False, False, False,  True,\n",
       "       False, False,  True, False,  True,  True, False, False, False,\n",
       "        True,  True, False, False, False, False, False,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True, False, False,  True,\n",
       "       False,  True, False, False,  True, False, False,  True, False,\n",
       "       False,  True,  True, False, False, False, False,  True, False,\n",
       "        True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True, False,  True,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "        True,  True,  True, False, False,  True, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True, False, False, False,\n",
       "       False,  True,  True,  True, False,  True,  True,  True, False,\n",
       "       False, False, False,  True,  True,  True,  True,  True, False,\n",
       "       False, False, False,  True,  True,  True, False, False, False,\n",
       "       False, False,  True, False, False,  True,  True,  True,  True,\n",
       "       False, False, False,  True,  True, False,  True, False,  True,\n",
       "       False, False,  True,  True,  True,  True, False,  True, False,\n",
       "        True, False,  True, False,  True,  True, False, False,  True,\n",
       "        True, False, False, False,  True, False,  True, False, False,\n",
       "       False,  True, False,  True,  True,  True, False,  True,  True,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True,  True,  True, False, False,  True,  True, False,  True,\n",
       "        True, False,  True, False,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True, False,  True, False,\n",
       "        True, False, False, False, False,  True,  True,  True,  True,\n",
       "        True, False, False,  True, False, False, False,  True, False,\n",
       "       False,  True,  True, False, False,  True, False, False, False,\n",
       "        True, False, False,  True, False, False, False, False, False,\n",
       "        True, False, False,  True, False, False, False,  True, False,\n",
       "        True, False, False,  True, False, False,  True,  True,  True,\n",
       "        True,  True, False,  True, False, False,  True,  True,  True,\n",
       "       False,  True,  True,  True, False, False,  True, False,  True,\n",
       "        True,  True, False,  True, False,  True, False, False,  True,\n",
       "       False, False, False, False,  True, False,  True, False, False,\n",
       "       False,  True, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True, False, False,  True, False, False, False, False,\n",
       "       False,  True, False, False,  True,  True,  True,  True, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "        True,  True,  True,  True, False,  True, False, False, False,\n",
       "       False, False,  True,  True, False, False,  True,  True, False,\n",
       "        True, False, False, False, False,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "        True,  True,  True, False,  True, False, False, False, False,\n",
       "       False, False,  True,  True,  True, False])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######### RANDOM FOREST\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score,recall_score\n",
    "acc = []\n",
    "f1_list = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    # Initialize Random Forest classifier with different number of trees\n",
    "    rf_clf = RandomForestClassifier(n_estimators=i, random_state=42)\n",
    "    \n",
    "    # Train the Random Forest classifier\n",
    "    rf_clf.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    preds = rf_clf.predict(x_val)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "    print(\"Accuracy (n_estimators={}): {:.2f}%\".format(i, accuracy * 100))\n",
    "    acc.append(accuracy)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_val, preds)\n",
    "    f1_list.append(f1)\n",
    "    print(\"F1 Score (n_estimators={}): {:.2f}\".format(i, f1))\n",
    "    \n",
    "    \n",
    "from sklearn.metrics import accuracy_score, f1_score,recall_score, precision_score, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "predict_and_plot(rf_clf, x_val, y_val)\n",
    "\n",
    "# predict_and_plot(knn_clf, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "F1 Score: 1.00\n",
      "1.0\n",
      "1.0\n",
      "AUC Score:1.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, False,  True, False, False, False,  True,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "        True, False,  True, False, False, False,  True,  True,  True,\n",
       "        True, False, False, False,  True, False,  True, False, False,\n",
       "       False,  True, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True,  True,  True,\n",
       "       False, False, False, False,  True,  True, False, False,  True,\n",
       "        True, False, False, False, False, False,  True,  True,  True,\n",
       "       False, False,  True,  True,  True, False,  True,  True,  True,\n",
       "       False,  True, False, False, False,  True,  True, False, False,\n",
       "       False, False, False,  True, False,  True, False,  True,  True,\n",
       "       False, False,  True,  True, False,  True, False,  True, False,\n",
       "       False,  True, False, False, False,  True, False,  True, False,\n",
       "       False,  True,  True, False,  True,  True, False, False,  True,\n",
       "       False,  True, False,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True, False,  True,  True, False,  True,\n",
       "        True, False,  True, False, False,  True, False,  True, False,\n",
       "        True, False,  True, False, False, False,  True, False,  True,\n",
       "        True, False, False, False, False,  True, False, False, False,\n",
       "        True,  True,  True,  True, False,  True,  True, False, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True, False,  True, False, False, False, False,  True, False,\n",
       "       False,  True, False, False,  True, False, False, False, False,\n",
       "       False, False,  True, False, False,  True,  True, False, False,\n",
       "        True, False, False, False, False, False, False,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "       False,  True, False, False,  True, False,  True,  True, False,\n",
       "       False,  True,  True, False, False, False, False,  True, False,\n",
       "       False, False, False,  True,  True, False,  True,  True, False,\n",
       "       False,  True, False,  True, False, False,  True, False,  True,\n",
       "        True, False, False, False,  True, False, False, False,  True,\n",
       "       False,  True, False, False, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False,  True, False,\n",
       "       False, False, False,  True, False,  True,  True, False,  True,\n",
       "       False, False, False, False,  True, False,  True,  True, False,\n",
       "        True,  True, False, False,  True,  True, False, False, False,\n",
       "        True, False,  True, False, False,  True, False,  True, False,\n",
       "       False,  True, False,  True, False,  True,  True, False,  True,\n",
       "       False,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        True, False, False, False, False, False,  True,  True, False,\n",
       "       False,  True, False, False,  True, False, False,  True, False,\n",
       "        True, False, False,  True,  True,  True, False,  True,  True,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "        True, False,  True, False,  True, False,  True, False, False,\n",
       "        True, False,  True,  True,  True,  True,  True, False, False,\n",
       "        True,  True, False, False, False,  True,  True,  True, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "        True,  True,  True,  True,  True,  True, False, False, False,\n",
       "       False, False, False,  True,  True, False, False, False, False,\n",
       "        True, False, False,  True,  True,  True, False,  True, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "        True,  True,  True, False, False, False, False,  True,  True,\n",
       "       False,  True,  True,  True, False, False, False, False,  True,\n",
       "       False, False,  True,  True, False,  True,  True, False,  True,\n",
       "       False, False, False, False,  True, False, False, False,  True,\n",
       "       False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "       False,  True, False,  True, False, False, False,  True, False,\n",
       "        True,  True, False, False, False,  True,  True,  True,  True,\n",
       "        True, False, False, False, False,  True,  True, False,  True,\n",
       "       False,  True, False, False,  True,  True,  True, False, False,\n",
       "       False, False,  True,  True,  True, False,  True,  True,  True,\n",
       "       False, False,  True,  True, False, False, False, False, False,\n",
       "        True, False, False, False,  True,  True,  True,  True, False,\n",
       "       False, False,  True, False, False,  True,  True, False, False,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "        True, False, False, False, False, False])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_and_plot(knn_clf, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "y_v = label\n",
    "y_v = to_categorical(y_v)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, y_v, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam as LegacyAdam  # Import the legacy Adam optimizer\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = models.Sequential()\n",
    "    model.add(Input(shape=(x_train.shape[1],)))\n",
    "\n",
    "    for i in range(hp.Int('layers', 2, 6)):\n",
    "        model.add(Dense(units=hp.Int('units_' + str(i), 32, 1024, step=32),\n",
    "                        activation=hp.Choice('act_' + str(i), ['relu', 'sigmoid'])))\n",
    "\n",
    "    model.add(Dense(v.N_CLASSES, activation='softmax', name='out'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=LegacyAdam(learning_rate=hp_learning_rate),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 09s]\n",
      "val_accuracy: 0.6775000095367432\n",
      "\n",
      "Best val_accuracy So Far: 0.8983333110809326\n",
      "Total elapsed time: 00h 04m 20s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, epochs=50, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 739us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_true = y_test\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       311\n",
      "           1       0.89      0.93      0.91       289\n",
      "\n",
      "    accuracy                           0.91       600\n",
      "   macro avg       0.91      0.91      0.91       600\n",
      "weighted avg       0.91      0.91      0.91       600\n",
      "\n",
      "[[277  34]\n",
      " [ 20 269]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_pred))\n",
    "print(metrics.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Add convolutional layers\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv2D(\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39madd(MaxPooling2D(pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv2D(\u001b[38;5;241m64\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assuming 'data' contains EEG signals and 'label' contains corresponding labels\n",
    "# Ensure data is formatted properly as a 3D array (samples, height, width)\n",
    "# For CNN, EEG signals might be represented as 2D images or spectrograms\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_categorical = to_categorical(y_train_encoded)\n",
    "y_test_categorical = to_categorical(y_test_encoded)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(data.shape[1], data.shape[2], 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output before feeding into fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout layer for regularization\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train_categorical, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test_categorical)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "# Predictions\n",
    "y_pred_categorical = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_categorical, axis=1)\n",
    "\n",
    "# Decode one-hot encoded labels back to original labels\n",
    "y_test_decoded = label_encoder.inverse_transform(np.argmax(y_test_categorical, axis=1))\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "print(classification_report(y_test_decoded, label_encoder.inverse_transform(y_pred)))\n",
    "print(confusion_matrix(y_test_decoded, label_encoder.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (3000, 96)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Add LSTM layer\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.2\u001b[39m))  \u001b[38;5;66;03m# Dropout for regularization\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Add another LSTM layer\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assuming 'data' contains EEG signals and 'label' contains corresponding labels\n",
    "# Ensure data is formatted properly as a 3D array (samples, timesteps, features)\n",
    "# For RNN, EEG signals might be represented as sequences of data points\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Define RNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(units=64, input_shape=(data.shape[1], data.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))  # Dropout for regularization\n",
    "\n",
    "# Add another LSTM layer\n",
    "model.add(LSTM(units=64, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train_categorical, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test_categorical)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_categorical = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_categorical, axis=1)\n",
    "\n",
    "# Decode one-hot encoded labels back to original labels\n",
    "y_test_decoded = label_encoder.inverse_transform(np.argmax(y_test_categorical, axis=1))\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "print(classification_report(y_test_decoded, label_encoder.inverse_transform(y_pred)))\n",
    "print(confusion_matrix(y_test_decoded, label_encoder.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_19 (Dense)            (None, 32)                3104      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5866 (22.91 KB)\n",
      "Trainable params: 5866 (22.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16706 (65.26 KB)\n",
      "Trainable params: 16706 (65.26 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 64), found shape=(None, 96)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train_categorical, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test set\u001b[39;00m\n\u001b[0;32m     65\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test_categorical)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filec3exwj5i.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 64), found shape=(None, 96)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 31, 31, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               1605760   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1624834 (6.20 MB)\n",
      "Trainable params: 1624834 (6.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 64, 64, 1), found shape=(None, 96)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train_categorical, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test set\u001b[39;00m\n\u001b[0;32m     50\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test_categorical)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filec3exwj5i.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 64, 64, 1), found shape=(None, 96)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assuming 'data' contains EEG signals and 'label' contains corresponding labels\n",
    "# Ensure data is formatted properly as a 4D array (samples, height, width, channels)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train_categorical, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test_categorical)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_categorical = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_categorical, axis=1)\n",
    "\n",
    "# Decode one-hot encoded labels back to original labels\n",
    "y_test_decoded = label_encoder.inverse_transform(np.argmax(y_test_categorical, axis=1))\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "print(classification_report(y_test_decoded, label_encoder.inverse_transform(y_pred)))\n",
    "print(confusion_matrix(y_test_decoded, label_encoder.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 288000 into shape (3000,64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3000\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 288000 into shape (3000,64)"
     ]
    }
   ],
   "source": [
    "data = data.reshape(3000,64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assuming 'data' contains reshaped EEG signals and 'label' contains corresponding labels\n",
    "# Ensure data is formatted properly as a 4D array (samples, height, width, channels)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(label))\n",
    "y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 1, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train_categorical, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test_categorical)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_categorical = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_categorical, axis=1)\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 32)                3104      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5866 (22.91 KB)\n",
      "Trainable params: 5866 (22.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer with the adjusted input shape\n",
    "model.add(Dense(32, activation='relu', input_shape=(96,)))  # Adjust input_shape to match the shape of your input data\n",
    "\n",
    "# Add more layers as needed\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model and specify loss function, optimizer, etc.\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_25 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16706 (65.26 KB)\n",
      "Trainable params: 16706 (65.26 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 64), found shape=(None, 96)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train_categorical, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test set\u001b[39;00m\n\u001b[0;32m     46\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test_categorical)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filec3exwj5i.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 64), found shape=(None, 96)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assuming 'data' contains EEG signals and 'label' contains corresponding labels\n",
    "# Ensure data is formatted properly as a 2D array (samples, features)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Define DNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add Dense layers with ReLU activation\n",
    "model.add(Dense(128, activation='relu', input_shape=(64,)))\n",
    "model.add(Dropout(0.5))  # Dropout layer for regularization\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add output layer with softmax activation for classification\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train_categorical, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test_categorical)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_categorical = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_categorical, axis=1)\n",
    "\n",
    "# Decode one-hot encoded labels back to original labels\n",
    "y_test_decoded = label_encoder.inverse_transform(np.argmax(y_test_categorical, axis=1))\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "print(classification_report(y_test_decoded, label_encoder.inverse_transform(y_pred)))\n",
    "print(confusion_matrix(y_test_decoded, label_encoder.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a0b16b431f91af56543167d2335ade6a4f69621936ac10d0388e1e58aabcd37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
